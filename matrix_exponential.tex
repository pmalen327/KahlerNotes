\section{The Exponential Map and Matrix Groups}
\subsection{The Matrix Exponential}
Given an $n \times n$ matrix $A$, we want to find a way to find $e^A$. We can actually
just do this with the usual power series:

\begin{equation}\label{eq:1}
    e^A = I_n + \sum_{p \ge 1} \frac{A^p}{p!} = \sum_{p \ge 0} \frac{A^p}{p!}
\end{equation}

Using an inductive proof, we can show that this is well defined. But we won't write
it out here.

\begin{boxex}{}{trigexample}
    Consider the matrix 
    \begin{equation*}
        A=
        \begin{pmatrix}
            0 & -\theta\\
            \theta & 0
        \end{pmatrix}
    \end{equation*}
    We want to find a way to express the powers $A^n$. We can factor out a $\theta$
    to see
    \begin{equation*}
        \begin{pmatrix}
            0 & -\theta\\
            \theta & 0
        \end{pmatrix}
        = \theta
        \begin{pmatrix}
            0 & -1\\
            1 & 0
        \end{pmatrix}
        \text{ and }
        \begin{pmatrix}
            0 & -\theta\\
            \theta & 0
        \end{pmatrix}^2
        =-\theta^2 I_2
    \end{equation*}
    Now, let
    \begin{equation*}
        J = 
        \begin{pmatrix}
            0 & -1\\
            1 & 0
        \end{pmatrix}
    \end{equation*}
    we have
    \begin{align*}
        A^{4n} &= \theta^{4n}I_2\\
        A^{4n+1} &= \theta^{4n+1}J\\
        A^{4n+2} &= -\theta^{4n+2}I_2
    \end{align*}
    and so on. This means we can now express $e^A$ as a power series
    \begin{equation*}
        e^A = I_2 + \frac{\theta}{1!}J - \frac{\theta^2}{2!}I_2 \hdots
    \end{equation*}
    Writing this out we will see that we actually get the power series for cosine
    and sine, thus
    \begin{equation*}
        e^A = \cos \theta I_2 + \sin \theta J
    \end{equation*}
    or equivalently
    \begin{equation*}
        e^A = 
        \begin{pmatrix}
            \cos \theta & -\sin \theta\\
            \sin \theta & \cos \theta
        \end{pmatrix}
    \end{equation*}
    So we see $e^A$ is in fact a rotation matrix.
\end{boxex}

This is actually a general fact. If we have a skew-symmetric matrix $A$, then $e^A$
is an orthogonal matrix with determinant $1$. In fact, EVERY
rotation matrix is of this form. To be explicit, the exponential map from the set
of skew-symmetric matrices to the set of rotation matrices is surjective. But note
that the exponential map is NOT surjective in general.

\begin{boxprop}{}{expprop}
    Let $A$ and $U$ be (real or complex) matrices and assume $U$ is invertible.
    Then
    \begin{equation*}
        e^{UAU^{-1}} = Ue^AU^{-1}
    \end{equation*}
\end{boxprop}

This is pretty obvious and its easily proven using an inductive proof. But I hate
induction so of course I will not include the proof here, although its only a few
lines. Now we will look at another important result that will be important when we
start to look at some spectral properties of the exponential map.

\begin{boxprop}{}{}
    Given any complex $n \times n$ matrix $A$, there is an invertible matrix $P$
    and an upper-triangular matrix $T$ such that
    \begin{equation*}
        A = PTP^{-1}
    \end{equation*}

    \begin{proof}
        (Sketch) Induct on $n$ if $f: \C \to \C$ is a linear map then there exists some basis
        with respect to $f$ which can be represented as an upper-triangular matrix.
    \end{proof}

    But note this proof is very technical.
\end{boxprop}

The exponential operator has some nice properties. Some are expected but some
properties that we might expect to hold are actually not true.

\begin{boxprop}{Properties of Exponential Operator}{}

    \begin{itemize}
        \item If $\{ \lambda_i \}_n$ are the eigenvalues of $A$, then $\{e^{\lambda_i}\}_n$
        are the eigenvalues of $e^A$
        \item $\det(e^A) = e^{ \text{tr} (A)}$
        \item If $A$ and $B$ commute under multplication, then $e^{A+B} = e^Ae^B$
    \end{itemize}
\end{boxprop}


\subsection{Matrix Lie Groups}
First, recall the ``usual'' matrix groups.
\begin{boxdef}{Common Lie Groups and Lie Algebras}{}
    \begin{itemize}
        \item $GL(n, \R)$: The group of all real invertible $n \times n$ matrices.
        This is the \textit{general linear group}.
        \item $SL(n, \R)$: The group of all real invertible $n \times n$ matrices
        with determinant $+1$. This is the \textit{special linear group}. Note that
        this is a subgroup of the general linear group.
        \item $O(n)$: The group of all real orthogonal $n\times n$ matrices. This is the
        \textit{orthogonal group}.
        \item $SO(n)$: The group of all real orthogonal $n\times n$ matrices with determinant
        $+1$. This is the \textit{special orthogonal group}. Note this is a subgroup
        of the orthogonal group.
        \item $\mathfrak{sl}(n, \R)$: The vector space of real $n \times n$ matrices with
        null trace.
        \item $\mathfrak{so}(n)$: The vector space of real skew-symmetric $n \times n$
        matrices.
    \end{itemize}
\end{boxdef}

The groups from above are more than just groups, they have additional topological
structure. They are topological spaces (viewed as subspaces of $\R^{n^2}$) with
smooth operations, specifically, the inverse and multplication operations are 
continuous. Further, they are in fact smooth manifolds (we will define this later).
These are examples of \textit{Lie groups}; groups that are simultaneously topological
spaces, and smooth manifolds. The above vector spaces are \textit{Lie algebras};
tangent spaces at the identity of the respective group. The algebraic structure
on Lie algebras is well defined, we will see later how far this defition extends.

\begin{boxdef}{Lie Bracket}{}
    The \textit{Lie bracket} of a Lie algebra is the commutator
    \begin{equation*}
        [A, B] = AB - BA
    \end{equation*}
\end{boxdef}

Note that if $A$ and $B$ commute, their Lie bracket is trivial. Later, we will see
how this corresponds to conservative vector fields on manifolds. There is a really
cool connection between Lie Groups and their Lie algebras, in fact, the exponential
map is a map from the Lie algebra to the Lie group.

\begin{align}\label{eq:2}
    \exp: & \ \mathfrak{so}(n) \to SO(n)\\
    \exp: & \ \mathfrak{sl}(n, \R) \to SL(n, \R)
\end{align}

This is really neat because it lets us parameterize the Lie group elements by the
Lie algebra elements, which sounds weird but it will be really convenient later.

Well what happened to the Lie algebras $\mathfrak{gl}(n, \R)$ and $\mathfrak{o}(n)$?
As it turns out, these Lie algebras are actually equivalent to some already familiar
vector spaces. It happens that $\mathfrak{gl}(n, \R)$ is just the vector space of
all real $n\times n$ matrices, while $\mathfrak{o}(n) = \mathfrak{so}(n)$.

Sometimes these maps can be explicity computed, giving rise to lots of application
in kinematics, robotics, etc.

\begin{boxprop}{Rodrigues Formula}{Rodrigues}
    The exponential map $\exp : \mathfrak{so}(3) \to SO(3)$ is given by
    \begin{equation*}
        e^A = I_3 + \frac{\sin \theta}{\theta}A + \frac{(1-\cos \theta)}{\theta^2}A^2
    \end{equation*}
    if $\theta \neq 0$, with $e^{0_3} = I_3$.

    \begin{proof}
        Notice that
        \begin{equation*}
            A^2 = -\theta^2 I_3 + B
        \end{equation*}
        and $AB=BA=0$.
        From there we can easily deduce the fact that
        \begin{equation*}
            A^3 = \theta^2A
        \end{equation*}
        Now we just write the power series for $e^A$
        \begin{equation*}
            e^A = I_3 + \sum_{p \ge 1}\frac{A^p}{p!}
        \end{equation*}
        Some painful calculation will give us the following
        \begin{align*}
            e^A &= I_3 + \sum_{p\ge 0}\frac{A^{2p+1}}{(2p+1)!} + \sum_{p\ge 1}\frac{A^{2p}}{(2p)!}\\
                &= I_3 + \sum_{p\ge 0}\frac{(-1)^p \theta^{2p}}{(2p+1)!}A + \sum_{p\ge 1}\frac{(-1)^{p-1}\theta^{2(p-1)}}{(2p)!}A\\
                &= I_3 + \frac{A}{\theta}\sum_{p\ge 0}\frac{(-1)^p \theta^{2p+1}}{(2p+1)!} - \frac{A^2}{\theta^2}\sum_{p\ge 1}\frac{(-1)^p\theta^{2p}}{(2p)!}\\
                &= I_3 + \frac{\sin\theta}{\theta}A - \frac{A^2}{\theta^2}\sum_{p\ge 0}\frac{(-1)^p\theta^{2p}}{(2p)!} + \frac{A^2}{\theta^2}\\
                &= I_3 + \frac{\sin\theta}{\theta}A + \frac{(1-\cos\theta)}{\theta^2}A^2
        \end{align*}
    \end{proof}
    It looks worse than it is, its really just moving things around to get the power
    series for sine and cosine. Although I do not want to do it again.
\end{boxprop}

\subsubsection{Symmetric and Positive Definite Matrices}
Now we will backtrack a little bit and look at another important class of matrices,
and consequently, groups. A matrix is \textit{positive semidefinite} if all of its
eigenvalues are nonnegative. The spectrum of positive definite matrices is used
extensively in optimization and convex analysis, it also has some neat geometric
implications we will see later. I assume familiarity of symmetric matries by now,
my first grade students could identity one. Now for some counterintuitive notation.
We denote the vector space (not necessarily group) of symmetric $n\times n$ matrices by $S(n)$
and the vector space of symmetric positive define matrices by $SPD(n)$. Yes I know
the notation is the same we used for matrix Lie groups, and we used the fraktur symbols for
the vector spaces and algebras. But just remember these are vector spaces, I will
try my best to remember myself and remind the reader. It is also not uncommon for
this notation to use boldface characters, it is however a pain to type sometimes.

\begin{boxprop}{}{}
    For every symmetric matrix $B$, the matrix $e^B$ is symmetric positive definite.
    For every symmetric positive definite matrix $A$, there is a unique symmetric
    matrix $B$ such that $A=e^B$.
    \begin{proof}
        Two pages and some technical linear algebra, I still don't understand it
        completely. 
    \end{proof}
\end{boxprop}





