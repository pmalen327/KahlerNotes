\section{The Exponential Map and Matrix Groups}
\subsection{The Matrix Exponential}
Given an $n \times n$ matrix $A$, we want to find a way to find $e^A$. We can actually
just do this with the usual power series:

\begin{equation}\label{eq:1}
    e^A = I_n + \sum_{p \ge 1} \frac{A^p}{p!} = \sum_{p \ge 0} \frac{A^p}{p!}
\end{equation}

Using an inductive proof, we can show that this is well defined. But we won't write
it out here.

\begin{boxex}{}{trigexample}
    Consider the matrix 
    \begin{equation*}
        A=
        \begin{pmatrix}
            0 & -\theta\\
            \theta & 0
        \end{pmatrix}
    \end{equation*}
    We want to find a way to express the powers $A^n$. We can factor out a $\theta$
    to see
    \begin{equation*}
        \begin{pmatrix}
            0 & -\theta\\
            \theta & 0
        \end{pmatrix}
        = \theta
        \begin{pmatrix}
            0 & -1\\
            1 & 0
        \end{pmatrix}
        \text{ and }
        \begin{pmatrix}
            0 & -\theta\\
            \theta & 0
        \end{pmatrix}^2
        =-\theta^2 I_2
    \end{equation*}
    Now, let
    \begin{equation*}
        J = 
        \begin{pmatrix}
            0 & -1\\
            1 & 0
        \end{pmatrix}
    \end{equation*}
    we have
    \begin{align*}
        A^{4n} &= \theta^{4n}I_2\\
        A^{4n+1} &= \theta^{4n+1}J\\
        A^{4n+2} &= -\theta^{4n+2}I_2
    \end{align*}
    and so on. This means we can now express $e^A$ as a power series
    \begin{equation*}
        e^A = I_2 + \frac{\theta}{1!}J - \frac{\theta^2}{2!}I_2 \hdots
    \end{equation*}
    Writing this out we will see that we actually get the power series for cosine
    and sine, thus
    \begin{equation*}
        e^A = \cos \theta I_2 + \sin \theta J
    \end{equation*}
    or equivalently
    \begin{equation*}
        e^A = 
        \begin{pmatrix}
            \cos \theta & -\sin \theta\\
            \sin \theta & \cos \theta
        \end{pmatrix}
    \end{equation*}
    So we see $e^A$ is in fact a rotation matrix.
\end{boxex}

This is actually a general fact. If we have a skew-symmetric matrix $A$, then $e^A$
is an orthogonal matrix with determinant $1$. In fact, EVERY
rotation matrix is of this form. To be explicit, the exponential map from the set
of skew-symmetric matrices to the set of rotation matrices is surjective. But note
that the exponential map is NOT surjective in general.

\begin{boxprop}{}{expprop}
    Let $A$ and $U$ be (real or complex) matrices and assume $U$ is invertible.
    Then
    \begin{equation*}
        e^{UAU^{-1}} = Ue^AU^{-1}
    \end{equation*}
\end{boxprop}

This is pretty obvious and its easily proven using an inductive proof. But I hate
induction so of course I will not include the proof here, although its only a few
lines. Now we will look at another important result that will be important when we
start to look at some spectral properties of the exponential map.

\begin{boxprop}{}{}
    Given any complex $n \times n$ matrix $A$, there is an invertible matrix $P$
    and an upper-triangular matrix $T$ such that
    \begin{equation*}
        A = PTP^{-1}
    \end{equation*}

    \begin{proof}
        (Sketch) Induct on $n$ if $f: \C \to \C$ is a linear map then there exists some basis
        with respect to $f$ which can be represented as an upper-triangular matrix.
    \end{proof}

    But note this proof is very technical.
\end{boxprop}

The exponential operator has some nice properties. Some are expected but some
properties that we might expect to hold are actually not true.

\begin{boxprop}{Properties of Exponential Operator}{}

    \begin{itemize}
        \item If $\{ \lambda_i \}_n$ are the eigenvalues of $A$, then $\{e^{\lambda_i}\}_n$
        are the eigenvalues of $e^A$
        \item $\det(e^A) = e^{ \text{tr} (A)}$
        \item If $A$ and $B$ commute under multplication, then $e^{A+B} = e^Ae^B$
    \end{itemize}
\end{boxprop}


\subsection{Matrix Lie Groups}
First, recall the ``usual'' matrix groups.
\begin{boxdef}{Common Lie Groups and Lie Algebras}{}
    \begin{itemize}
        \item $GL(n, \R)$: The group of all real invertible $n \times n$ matrices.
        This is the \textit{general linear group}.
        \item $SL(n, \R)$: The group of all real invertible $n \times n$ matrices
        with determinant $+1$. This is the \textit{special linear group}. Note that
        this is a subgroup of the general linear group.
        \item $O(n)$: The group of all real orthogonal $n\times n$ matrices. This is the
        \textit{orthogonal group}.
        \item $SO(n)$: The groups of all real orthogonal $n\times n$ matrices with determinant
        $+1$. This is the \textit{special orthogonal group}. Note this is a subgroup
        of the orthogonal group.
        \item $\mathfrak{sl}(n, \R)$: The vector space of real $n \times n$ matrices with
        null trace.
        \item $\mathfrak{so}(n)$: The vector space of real skew-symmetric $n \times n$
        matrices.
    \end{itemize}
\end{boxdef}

The groups from above are more than just groups, they have additional topological
structure. They are topological spaces (viewed as subspaces of $\R^{n^2}$) with
smooth operations, specifically, the inverse and multplication operations are 
continuous. Further, they are in fact smooth manifolds (we will define this later).
These are examples of \textit{Lie groups}; groups that are simultaneously topological
spaces, and smooth manifolds. The above vector spaces are \textit{Lie algebras};
tangent spaces at the identity of the respective group. The algebraic structure
on Lie algebras is well defined, we will see later how far this defition extends.

\begin{boxdef}{Lie bracket}{}
    The \textit{Lie bracket} of a Lie algebra is the commutator
    \begin{equation*}
        [A, B] = AB - BA
    \end{equation*}
\end{boxdef}

Note that if $A$ and $B$ commute, their Lie bracket is trivial. Later, we will see
how this corresponds to conservative vector fields on manifolds. There is a really
cool connection between Lie Groups and their Lie algebras, in fact, the exponential
map is a map from the Lie algebra to the Lie group.
\begin{align}\label{eq:2}
    \exp: & \ \mathfrak{so}(n) \to SO(n)\\
    \exp: & \ \mathfrak{sl}(n, \R) \to SL(n, \R)
\end{align}
This is really neat because it lets us parameterize the Lie group elements by the
Lie algebra elements, which sounds weird but it will be really convenient later.

Well what happened to the Lie algebras $\mathfrak{gl}(n, \R)$ and $\mathfrak{o}(n)$?
As it turns out, these Lie algebras are actually equivalent to some already familiar
vector spaces. It happens that $\mathfrak{gl}(n, \R)$ is just the vector space of
all real $n\times n$ matrices, while $\mathfrak{o}(n) = \mathfrak{so}(n)$.






